{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hatemongers ride on echo chambers to escalate hate speech diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import davidson_model\n",
    "import fountana_model\n",
    "import waseem_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from dateutil import parser\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\", \n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    spell_correct_elong=False,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe specifications\n",
    "\n",
    "Our code works on any social media platform, given the data is modelled into a dataframe as specified below, where each row of the dataframe refers to a post:\n",
    "\n",
    "1. `id` -> ID of the post\n",
    "2. `content` -> Original content of the post\n",
    "3. `content_ek` -> The preprocessed textual content for the post\n",
    "4. `created_at` -> Time of posting of the post\n",
    "5. `parent` -> The ID of the post if this post is a retweet/comment/reblog to another post else None\n",
    "6. `account_id` -> ID of the user who posted this post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_ek'] = df.content.progress_apply(lambda x: \" \".join(text_processor.pre_process_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), 1000)):\n",
    "    davidson_scores.append(davidson_model.score_set(df.content_ek.values[i:i+1000], only_hate = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waseem_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), 1000)):\n",
    "    waseem_scores.append(waseem_model.score_set(df.content_ek.values[i:i+1000], only_hate = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), 500)):\n",
    "    founta_scores.append(fountana_model.score_set(df.content_ek.values[i:i+500], only_hate = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_score_list = []\n",
    "waseem_score_list = []\n",
    "founta_score_list = []\n",
    "\n",
    "for i in davidson_scores:\n",
    "    for j in i:\n",
    "        davidson_score_list.append(j)\n",
    "        \n",
    "for i in waseem_scores:\n",
    "    for j in i:\n",
    "        waseem_score_list.append(j)\n",
    "\n",
    "for i in founta_scores:\n",
    "    for j in i:\n",
    "        founta_score_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hate_dson'] = davidson_score_list\n",
    "df['hate_waseem'] = waseem_score_list\n",
    "df['hate_founta'] = founta_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(['h_score_davidson', 'h_type', 'mentions', 'tags', 'emojis', 'in_reply_to_id', 'in_reply_to_account_id', 'replies_count', 'reblogs_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['d_bin'] = np.array(np.array(davidson_score_list) >= thresh[0], dtype = 'int')\n",
    "df_dropped['w_bin'] = np.array(np.array(waseem_score_list) >= thresh[1], dtype = 'int')\n",
    "df_dropped['f_bin'] = np.array(np.array(founta_score_list) >= thresh[2], dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['cumm_hate'] = df_dropped.d_bin + df_dropped.w_bin + df_dropped.f_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['cumm_hate'] = df_dropped['cumm_hate'].replace(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.cumm_hate.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['avg_score'] = (df_dropped.hate_dson + df_dropped.hate_waseem + df_dropped.hate_founta)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.boxplot(column = ['hate_dson', 'hate_waseem', 'hate_founta', 'avg_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['hate'] = np.array(df_dropped.hate_dson > 0.4 ,dtype='uint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hate = np.where(df_dropped.hate_dson < 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_hate = np.where((df_dropped.hate_dson >= 0.3) & (df_dropped.hate_dson < 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_hate = np.where(df_dropped.hate_dson >= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_preds = np.zeros(len(df_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_preds[no_hate] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_preds[mid_hate] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_preds[high_hate] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_dict = dict(Counter(list(hate_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_dropped[['cumm_hate']], x='cumm_hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_dropped['cumm_hate'], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casc_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df_dropped.iloc):\n",
    "    casc_graph.add_node(i.id, hate_score = [i.hate_dson, i.hate_waseem, i.hate_founta, i.avg_score], hate_bin = [i.d_bin, i.w_bin, i.f_bin], cumm_hate = i.cumm_hate, hate = i.hate, time = parser.parse(i.created_at), parent = pd.isna(float(i.parent)), cumm_hate_2 = 0 if i.hate_dson < 0.2 else 1 if i.hate_dson < 0.5 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df_dropped.iloc):\n",
    "    if not pd.isna(float(i.parent)):\n",
    "        casc_graph.add_edge(i.id, i.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascs = [i for i in nx.connected_components(casc_graph) if len(i) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascs_sg = [casc_graph.subgraph(i) for i in cascs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cascade subgraphs\n",
    "\n",
    "CAS = {}\n",
    "for i in cascs_sg:\n",
    "    for node in i.nodes:\n",
    "        if i.nodes[node].get('parent', False):\n",
    "            CAS[node] = i\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_scores_0 = []\n",
    "hate_scores_1 = []\n",
    "hate_scores_2 = []\n",
    "\n",
    "for i in CAS:\n",
    "    source_hate = CAS[i].nodes[i].get('cumm_hate_2', -1)\n",
    "    if source_hate != -1:\n",
    "        for j in CAS[i].nodes:\n",
    "            if i!=j:\n",
    "                if source_hate == 0:\n",
    "                    hate_scores_0.append(CAS[i].nodes[j].get('hate', 1))\n",
    "                elif source_hate == 1:\n",
    "                    hate_scores_1.append(CAS[i].nodes[j].get('hate', 1))\n",
    "                else:\n",
    "                    hate_scores_2.append(CAS[i].nodes[j].get('hate', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_fracs = [[0,0,0], [1,0,0], [2,0,0]]\n",
    "interaction_fracs[0][1] = hate_scores_0.count(0)/(hate_scores_0.count(0) + hate_scores_0.count(1))\n",
    "interaction_fracs[0][2] = hate_scores_0.count(1)/(hate_scores_0.count(0) + hate_scores_0.count(1))\n",
    "interaction_fracs[1][1] = hate_scores_1.count(0)/(hate_scores_1.count(0) + hate_scores_1.count(1))\n",
    "interaction_fracs[1][2] = hate_scores_1.count(1)/(hate_scores_1.count(0) + hate_scores_1.count(1))\n",
    "interaction_fracs[2][1] = hate_scores_2.count(0)/(hate_scores_2.count(0) + hate_scores_2.count(1))\n",
    "interaction_fracs[2][2] = hate_scores_2.count(1)/(hate_scores_2.count(0) + hate_scores_2.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df = pd.DataFrame(arr, columns=['source', 'non-hate', 'hate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting fraction of interactions grouped by typee of source post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['low', 'medium', 'high']\n",
    "non_hate = plotting_df['non-hate'].values\n",
    "hate = plotting_df['hate'].values\n",
    "\n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, non_hate, 0.4, label = 'non-hate')\n",
    "plt.bar(X_axis + 0.2, hate, 0.4, label = 'hate')\n",
    "  \n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Source type\")\n",
    "plt.ylabel(\"Fraction of interactions\")\n",
    "plt.title(\"Hate attracted by type of source\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['low', 'medium', 'high']\n",
    "total = plotting_df['non-hate'].values/80740 + plotting_df['hate'].values/80740\n",
    "  \n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, total, 0.4)\n",
    "plt.bar(X_axis + 0.2, hate, 0.4, label = 'hate')\n",
    "  \n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Source type\")\n",
    "plt.ylabel(\"Fraction of interactions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['cumm_hate_2'] = df_dropped['hate_dson'].progress_apply(lambda x: 0 if x < 0.2 else 1 if x < 0.5 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_author = {}\n",
    "\n",
    "for i in tqdm(df_dropped.iloc):\n",
    "    post_author[i.id] = i.account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_hate = {}\n",
    "\n",
    "for i in tqdm(df_dropped.iloc):\n",
    "    author_hate[i.account_id] = author_hate.get(i.account_id, []) + [i.cumm_hate_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df_dropped.iloc):\n",
    "    G.add_node(post_author[i.id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df_dropped.iloc):\n",
    "    if not pd.isna(float(i.parent)) and post_author.get(i.parent, False):\n",
    "        G.add_edge(post_author[i.id], post_author[i.parent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G.subgraph(max(nx.connected_components(G), key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_core = nx.core_number(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_hate_classify = {}\n",
    "for a in author_hate:\n",
    "    hate_counts = dict(Counter(author_hate[a]))\n",
    "    hateful = hate_counts.get(1, 0) + hate_counts.get(2, 0)\n",
    "    if hateful >= 4:\n",
    "        author_hate_classify[a] = 2\n",
    "    elif hateful >= 2 and hateful < 4:\n",
    "        author_hate_classify[a] = 1\n",
    "    else:\n",
    "        author_hate_classify[a] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_hate = {}\n",
    "for a in author_core:\n",
    "    c = author_core[a]\n",
    "    core_hate[c] = core_hate.get(c, [0, 0, 0])\n",
    "    core_hate[c][author_hate_classify[a]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hate_core_plots():\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "    sns.despine(top=True, right=True)\n",
    "    L = len(core_hate.keys()) - 1\n",
    "    non_hate  = [0]*L\n",
    "    med_hate  = [0]*L\n",
    "    high_hate = [0]*L\n",
    "    combinedCount = 3\n",
    "    iterator = 0\n",
    "    for c in range(1, L+1, combinedCount):\n",
    "        total = 0\n",
    "        for k in range(combinedCount):\n",
    "            total += sum(core_hate.get(c+k, [0]))\n",
    "\n",
    "        for k in range(combinedCount):\n",
    "            non_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[0]\n",
    "            med_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[1]\n",
    "            high_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[2]\n",
    "        iterator += 1\n",
    "    X_axis = np.arange(iterator)\n",
    "    width = 0.3\n",
    "    non_hate = non_hate[:iterator]\n",
    "    med_hate = med_hate[:iterator]\n",
    "    high_hate = high_hate[:iterator]\n",
    "    plt.bar(X_axis - width, non_hate, width, label = 'Non hate')\n",
    "    plt.bar(X_axis, med_hate, width, label='Medium hate')\n",
    "    plt.bar(X_axis + width, high_hate, width, label = 'High hate')\n",
    "    plt.xticks(X_axis, range(1, iterator+1))\n",
    "    plt.xlabel(\"Core\")\n",
    "    plt.ylabel(\"Fraction of users in core\")\n",
    "    plt.title(\"Hateful users in core decomposition of reddit network\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_core_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting core decomposition and distribution of type of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hate_core_plots():\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.figure(figsize=(10, 6), dpi=80)\n",
    "    sns.despine(top=True, right=True)\n",
    "    L = len(core_hate.keys()) - 1\n",
    "    non_hate  = [0]*L\n",
    "    med_hate  = [0]*L\n",
    "    high_hate = [0]*L\n",
    "    combinedCount = 3\n",
    "    iterator = 0\n",
    "    for c in range(1, L+1, combinedCount):\n",
    "        total = 0\n",
    "        for k in range(combinedCount):\n",
    "            total += sum(core_hate.get(c+k, [0]))\n",
    "\n",
    "        for k in range(combinedCount):\n",
    "            non_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[0]/total\n",
    "            med_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[1]/total\n",
    "            high_hate[iterator] += core_hate.get(c+k, [0, 0, 0])[2]/total\n",
    "        iterator += 1\n",
    "    X_axis = np.arange(iterator)\n",
    "    width = 0.3\n",
    "    non_hate = non_hate[:iterator]\n",
    "    med_hate = med_hate[:iterator]\n",
    "    high_hate = high_hate[:iterator]\n",
    "    plt.bar(X_axis - width, non_hate, width, label = 'Non hate')\n",
    "    plt.bar(X_axis, med_hate, width, label='Medium hate')\n",
    "    plt.bar(X_axis + width, high_hate, width, label = 'High hate')\n",
    "    plt.xticks(X_axis, range(1, iterator+1))\n",
    "    plt.xlabel(\"Core\")\n",
    "    plt.ylabel(\"Fraction of users in core\")\n",
    "    plt.title(\"Hateful users in core decomposition of reddit network\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_core_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_auth_0 = []\n",
    "hate_auth_1 = []\n",
    "hate_auth_2 = []\n",
    "\n",
    "for i in CAS:\n",
    "    source_hate = author_hate_classify.get(post_author[i], -1)\n",
    "    if source_hate != -1:\n",
    "        for j in CAS[i].nodes:\n",
    "            if i!=j:\n",
    "                if source_hate == 0:\n",
    "                    hate_auth_0.append(CAS[i].nodes[j].get('hate', 1))\n",
    "                elif source_hate == 1:\n",
    "                    hate_auth_1.append(CAS[i].nodes[j].get('hate', 1))\n",
    "                else:\n",
    "                    hate_auth_2.append(CAS[i].nodes[j].get('hate', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_int_fracs = [[0,0,0], [1,0,0], [2,0,0]]\n",
    "user_int_fracs[0][1] = hate_auth_0.count(0)/(hate_auth_0.count(0) + hate_auth_0.count(1))\n",
    "user_int_fracs[0][2] = hate_auth_0.count(1)/(hate_auth_0.count(0) + hate_auth_0.count(1))\n",
    "user_int_fracs[1][1] = hate_auth_1.count(0)/(hate_auth_1.count(0) + hate_auth_1.count(1))\n",
    "user_int_fracs[1][2] = hate_auth_1.count(1)/(hate_auth_1.count(0) + hate_auth_1.count(1))\n",
    "user_int_fracs[2][1] = hate_auth_2.count(0)/(hate_auth_2.count(0) + hate_auth_2.count(1))\n",
    "user_int_fracs[2][2] = hate_auth_2.count(1)/(hate_auth_2.count(0) + hate_auth_2.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df = pd.DataFrame(arr, columns=['source', 'non-hate', 'hate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting fraction of interactions grouped by typee of source user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['low', 'medium', 'high']\n",
    "non_hate = plotting_df['non-hate'].values\n",
    "hate = plotting_df['hate'].values\n",
    "  \n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, non_hate, 0.4, label = 'non-hate')\n",
    "plt.bar(X_axis + 0.2, hate, 0.4, label = 'hate')\n",
    "  \n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Source user type\")\n",
    "plt.ylabel(\"Fraction of interactions\")\n",
    "plt.title(\"Hate attracted by type of source user\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_dict = {}\n",
    "for i in tqdm(CAS):\n",
    "    descendant = nx.descendants(casc_graph, i)\n",
    "    parents_dict[i] = [descendant, len(descendant)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_sizes = []\n",
    "for v in parents_dict.values():\n",
    "    cascade_sizes.append(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 20\n",
    "filtered_dict = {}\n",
    "for p in parents_dict:\n",
    "    if parents_dict[p][1] >= min_size:\n",
    "        filtered_dict[p] = deepcopy(parents_dict[p])\n",
    "\n",
    "key = list(filtered_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from scipy.special import softmax\n",
    "\n",
    "def get_timeseries_for_cascade(p):\n",
    "    descendants = filtered_dict[p][0]\n",
    "    info = []\n",
    "    for d in descendants:\n",
    "        info_dict = casc_graph.nodes[d]\n",
    "        info_dict['id'] = d\n",
    "        info.append(info_dict)\n",
    "    df = pd.DataFrame.from_dict(info)\n",
    "    vals = list(df[['time', 'id']].values)\n",
    "    vals.append([casc_graph.nodes[p]['time'], p])\n",
    "    vals = np.array(vals)\n",
    "    sort_key = vals[:,0].argsort()\n",
    "    d = timedelta(days = 2)\n",
    "    filter = (casc_graph.nodes[p]['time'] + d).timestamp()\n",
    "    filtered_key = np.array([i.timestamp() for i in vals[sort_key][:, 0]]) < filter\n",
    "    vals = vals[sort_key][filtered_key]\n",
    "    ts = vals\n",
    "    return ts, vals[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "d = timedelta(days = 2)\n",
    "kernel_size = 12\n",
    "thresh=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hate(text):\n",
    "    return softmax(model(**tokenizer(text, return_tensors=\"pt\")).logits.detach().numpy())\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def plot_zero_crossings(key):\n",
    "    time_series, ids = get_timeseries_for_cascade(key)\n",
    "    T = list(time_series[:, 0].flatten() - time_series[0, 0])\n",
    "    T = [0] + [i.total_seconds() for i in T] + [0]\n",
    "    laplacian_T = []\n",
    "    kernel_size = 5\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    vel_T = []\n",
    "    for i in range(1, len(T) - 1):\n",
    "        laplacian_T.append((T[i+1] + T[i-1] - 2*T[i]))\n",
    "        vel_T.append((T[i+1] - T[i]))\n",
    "    laplacian_T = laplacian_T[:-1]\n",
    "    savgol_smooth = savgol_filter(laplacian_T, 5, 3)\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    mean_smooth = np.convolve(savgol_smooth, kernel, mode='same')\n",
    "    zero_crossings = np.where(np.diff(np.sign(mean_smooth)))[0]\n",
    "    print(len(zero_crossings))\n",
    "    hate_cross_1 = []\n",
    "    hate_cross_0 = []\n",
    "    hate_cross_2 = []\n",
    "    for z in zero_crossings:\n",
    "        hate_cross_0.append(casc_graph.nodes[ids[z-1]].get('hate', 0))\n",
    "        \n",
    "        hate_cross_1.append(casc_graph.nodes[ids[z]].get('hate', 0))\n",
    "        \n",
    "        hate_cross_2.append(casc_graph.nodes[ids[z+1]].get('hate', 0))\n",
    "\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.plot(range(len(mean_smooth)), mean_smooth)\n",
    "    plt.plot(zero_crossings, np.zeros(len(zero_crossings)), 'x')\n",
    "    plt.show()\n",
    "    return zero_crossings, Counter(hate_cross_0).get(1, 0), Counter(hate_cross_1).get(1, 0), Counter(hate_cross_2).get(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(filtered_dict.keys())[10] \n",
    "plot_zero_crossings(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_vals = [{}, {}, {}]\n",
    "failed = 0\n",
    "\n",
    "for i in CAS:\n",
    "    desc = nx.shortest_path_length(CAS[i], i)\n",
    "    levels = {}\n",
    "    base_time = CAS[i].nodes[i]['time'].timestamp()\n",
    "    \n",
    "    for d in desc:\n",
    "        levels[desc[d]] = levels.get(desc[d], float('inf'))\n",
    "        if (levels[desc[d]] > CAS[i].nodes[d]['time'].timestamp()):\n",
    "            levels[desc[d]] = CAS[i].nodes[d]['time'].timestamp()\n",
    "    \n",
    "    source_hate = CAS[i].nodes[i].get('cumm_hate', -1)\n",
    "    \n",
    "    if source_hate != -1:\n",
    "        for l in levels:\n",
    "            cdf_vals[source_hate][l] = cdf_vals[0].get(l, []) + [(levels[l] - base_time)]\n",
    "            \n",
    "title = \"CDF velocity based on source post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_medians = [[], [], []]\n",
    "cdf_mean = [[], [], []]\n",
    "for l in range(len(cdf_vals)):\n",
    "    L = cdf_vals[l]\n",
    "    for key in range(len(L.keys())):\n",
    "        if len(L[key]) > 10:\n",
    "            cdf_medians[l].append(np.median(L[key]))\n",
    "            cdf_mean[l].append(np.mean(L[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=100)\n",
    "plt.xlabel('Depth of cascade')\n",
    "plt.ylabel('Seconds after original post')\n",
    "plt.yscale(\"log\")\n",
    "labels = ['non-hate', 'medium-hate', 'high-hate']\n",
    "for l in range(0, 3, 2):\n",
    "    X = cdf_medians[l]\n",
    "    p = np.arange(len(X))\n",
    "    plt.step(p, X, label=labels[l])\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_vals = [{}, {}, {}]\n",
    "failed = 0\n",
    "\n",
    "for i in CAS:\n",
    "    desc = nx.shortest_path_length(CAS[i], i)\n",
    "    levels = {}\n",
    "    base_time = CAS[i].nodes[i]['time'].timestamp()\n",
    "    \n",
    "    for d in desc:\n",
    "        levels[desc[d]] = levels.get(desc[d], float('inf'))\n",
    "        if (levels[desc[d]] > CAS[i].nodes[d]['time'].timestamp()):\n",
    "            levels[desc[d]] = CAS[i].nodes[d]['time'].timestamp()\n",
    "    \n",
    "    source_hate = author_hate_classify.get(post_author[i], -1)\n",
    "    \n",
    "    if source_hate != -1:\n",
    "        for l in levels:\n",
    "            cdf_vals[source_hate][l] = cdf_vals[0].get(l, []) + [(levels[l] - base_time)]\n",
    "            \n",
    "title = \"CDF velocity based on source user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_medians = [[], [], []]\n",
    "cdf_mean = [[], [], []]\n",
    "for l in range(len(cdf_vals)):\n",
    "    L = cdf_vals[l]\n",
    "    for key in range(len(L.keys())):\n",
    "        if len(L[key]) > 10:\n",
    "            cdf_medians[l].append(np.median(L[key]))\n",
    "            cdf_mean[l].append(np.mean(L[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=100)\n",
    "plt.xlabel('Depth of cascade')\n",
    "plt.ylabel('Seconds after original post')\n",
    "plt.yscale(\"log\")\n",
    "labels = ['non-hate', 'medium-hate', 'high-hate']\n",
    "for l in range(0, 3, 2):\n",
    "    X = cdf_medians[l]\n",
    "    p = np.arange(len(X))\n",
    "    plt.step(p, X, label=labels[l])\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w, t='valid'):\n",
    "    return np.convolve(x, np.ones(w), t) / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "failed = 0\n",
    "\n",
    "for i in tqdm(CAS):\n",
    "    desc = nx.shortest_path_length(CAS[i], i)\n",
    "    levels = {}\n",
    "    base_time = CAS[i].nodes[i]['time'].timestamp()\n",
    "    timelines = []\n",
    "    content_arr = []\n",
    "    for d in desc:\n",
    "        timelines.append(CAS[i].nodes[i]['time'].timestamp() - base_time)\n",
    "        content_arr.append(d)\n",
    "    timelines = np.array(timelines)\n",
    "    content_arr = np.array(content_arr)\n",
    "    sorter = np.argsort(timelines)\n",
    "    timelines = timelines[sorter]\n",
    "    content_arr = timelines[sorter]\n",
    "    \n",
    "    filt = timelines < 86400\n",
    "    timelines = timelines[filt]\n",
    "    content_arr = content_arr[filt]\n",
    "    vel = np.gradient(np.arange(len(content_arr)), timelines)\n",
    "    acc = np.gradient(vel, timelines)\n",
    "    zero_crossings = np.where(np.diff(np.sign(acc)))[0]\n",
    "    results[i] = [timelines, zero_crossings, CAS[i].nodes[i]['hate'], author_hate_classify[post_author[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = np.zeros((len(results), 5))\n",
    "counter = 0\n",
    "for i in tqdm(results):\n",
    "    mappings[counter][0] = len(results[i][0])\n",
    "    mappings[counter][1] = len(results[i][1])\n",
    "    mappings[counter][2] = results[i][2]\n",
    "    mappings[counter][3] = results[i][3]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hate_zero = mappings[:, 2] == 0\n",
    "hate_zero = mappings[:, 2] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hate_zero_user = mappings[:, 3] == 0\n",
    "hate_zero_user = mappings[:, 3] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[no_hate_zero][:, 0].reshape(-1, 1)\n",
    "y = mappings[no_hate_zero][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_non_hate = reg.predict(X)\n",
    "X_non_hate = mappings[no_hate_zero][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[no_hate_zero][:, 0], mappings[no_hate_zero][:, 1], 'o')\n",
    "plt.plot(mappings[no_hate_zero][:, 0], preds_non_hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[hate_zero][:, 0].reshape(-1, 1)\n",
    "y = mappings[hate_zero][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_hate = reg.predict(X)\n",
    "X_med_hate = mappings[hate_zero][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[hate_zero][:, 0], mappings[hate_zero][:, 1], 'o')\n",
    "plt.plot(mappings[hate_zero][:, 0], preds_hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.plot(mappings[no_hate_zero][:, 0], preds_non_hate, label='non-hate')\n",
    "plt.plot(mappings[hate_zero][:, 0], preds_hate, label='high-hate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[no_hate_zero_user][:, 0].reshape(-1, 1)\n",
    "y = mappings[no_hate_zero_user][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_non_hate_user = reg.predict(X)\n",
    "X_non_hate_user = mappings[no_hate_zero_user][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[no_hate_zero_user][:, 0], mappings[no_hate_zero_user][:, 1], 'o')\n",
    "plt.plot(mappings[no_hate_zero_user][:, 0], preds_non_hate_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[hate_zero_user][:, 0].reshape(-1, 1)\n",
    "y = mappings[hate_zero_user][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_hate_user = reg.predict(X)\n",
    "X_hate_user = mappings[hate_zero_user][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[hate_zero_user][:, 0], mappings[hate_zero_user][:, 1], 'o')\n",
    "plt.plot(mappings[hate_zero_user][:, 0], preds_hate_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.plot(mappings[hate_zero_user][:, 0], preds_hate_user, label='high-hate user')\n",
    "plt.plot(mappings[no_hate_zero_user][:, 0], preds_non_hate_user, label='non-hate user')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echo Chamber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_dropped[df_dropped.id.isin([i for i in CAS])].content_ek.values\n",
    "embeds = np.zeros((len(text), 512))\n",
    "for i in tqdm(range(0, len(text), 16)):\n",
    "    embeds[i: i+16] = embed(text[i: i+16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_embeds = PCA(n_components=256).fit_transform(embeds)\n",
    "print(\"PCA_done\")\n",
    "standard_embedding = trans = umap.UMAP(n_neighbors=5, n_components=64).fit_transform(red_embeds)\n",
    "print(\"Umap done\")\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], s=0.1, cmap='Spectral');\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=3).fit(standard_embedding)\n",
    "print(\"Clustering done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = df_dropped[df_dropped.id.isin([i for i in CAS])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['labels'] = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clusterer.labels_, bins=len(Counter(roots.labels)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(standard_embedding[:, 6], standard_embedding[:, 7], s=20, linewidth=0, c=clusterer.labels_, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo = roots[['id', 'account_id', 'labels', 'hate', 'cumm_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {}\n",
    "for i in tqdm(roots[['id', 'labels']].values):\n",
    "    nodes = CAS[i[0]].nodes\n",
    "    id_to_label[i[0]] = i[1]\n",
    "    for n in nodes:\n",
    "        id_to_label[n] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_label = {}\n",
    "label_to_user = {}\n",
    "for i in tqdm(id_to_label):\n",
    "    auth = post_author[i]\n",
    "    label_to_user[id_to_label[i]] = label_to_user.get(id_to_label[i], dict())\n",
    "    label_to_user[id_to_label[i]][auth] = label_to_user[id_to_label[i]].get(auth, 0)\n",
    "    label_to_user[id_to_label[i]][auth] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_keys = []\n",
    "for l in label_to_user:\n",
    "    for a in label_to_user[l]:\n",
    "        if label_to_user[l][a] < 2:\n",
    "            remove_keys.append((l, a))\n",
    "for r in remove_keys:\n",
    "    l, a = r\n",
    "    del label_to_user[l][a]\n",
    "\n",
    "for l in label_to_user:\n",
    "    if l != -1:\n",
    "        for a in label_to_user[l]:\n",
    "            user_to_label[a] = user_to_label.get(a, dict())\n",
    "            user_to_label[a][l] = label_to_user[l][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "target_users = {}\n",
    "for u in user_to_label:\n",
    "    if len(user_to_label[u]) >= 2:\n",
    "        target_users[u] = set(user_to_label[u].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_ids = list(target_users.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches_4 = {}\n",
    "possible_matches_2 = {}\n",
    "possible_matches_3 = {}\n",
    "count = 0\n",
    "for t in tqdm(range(len(target_user_ids))):\n",
    "    for u in range(t+1, len(target_user_ids)):\n",
    "        tuser = target_user_ids[t]\n",
    "        uuser = target_user_ids[u]\n",
    "        match_key = tuple(sorted(list(target_users[tuser].intersection(target_users[uuser]))))\n",
    "        if len(match_key) >= 2:\n",
    "            possible_matches_2[match_key] = possible_matches_2.get(match_key, set())\n",
    "            possible_matches_2[match_key].add(tuser)\n",
    "            possible_matches_2[match_key].add(uuser)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4), dpi=80)\n",
    "counter = 0\n",
    "labels = [\"min_2\", \"min_3\", \"min_4\"]\n",
    "consider_echo_store = {\"min_2\": {}, \"min_3\": {}, \"min_4\": {}}\n",
    "for P in [possible_matches_2, possible_matches_3, possible_matches_4]:\n",
    "    thresh_to_size = []\n",
    "    for thresh in range(4, 21, 2):\n",
    "        consider_echo = {}\n",
    "        sizes_echo = []\n",
    "        for p in P:\n",
    "            if (len(P[p]) >= thresh):\n",
    "                sizes_echo.append(len(P[p]))\n",
    "                consider_echo[p] = P[p]\n",
    "        thresh_to_size.append([thresh, len(deepcopy(consider_echo))])\n",
    "        consider_echo_store[labels[counter]][thresh] = deepcopy(consider_echo)\n",
    "    thresh_to_size = np.array(thresh_to_size)\n",
    "    plt.plot(thresh_to_size[:, 0], thresh_to_size[:, 1], label=labels[counter])\n",
    "    counter += 1F\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.xlabel('Size of echo chamber')\n",
    "plt.ylabel('CDF')\n",
    "for l in range(0, 1, 2):\n",
    "    X = sorted(sizes_echo)\n",
    "    p = np.arange(len(X))/len(X)\n",
    "    plt.step(X, p)\n",
    "    plt.title('Size of echo chamber')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_echo = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consider_echo = consider_echo_store['min_2'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in consider_echo:\n",
    "    hate_echo[key] = {0: 0, 1: 0, 2: 0}\n",
    "    for user in consider_echo[key]:\n",
    "        try:\n",
    "            hate_echo[key][author_hate_classify[user]] += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity = []\n",
    "secondpurity = []\n",
    "hate_count = 0\n",
    "non_hate_count = 0\n",
    "uniform = []\n",
    "non_uniform = []\n",
    "for h in hate_echo:\n",
    "    E = hate_echo[h]\n",
    "    hate_counter = 0\n",
    "    if (E[0] + E[1]) <= E[2]:\n",
    "        purity.append(1 - (E[0]+E[1] + 0.00001)/(E[2] + 0.00001))\n",
    "        hate_count += 1\n",
    "        hate_counter += 1\n",
    "    else:\n",
    "        purity.append(1 - (E[2] + 0.00001)/(E[0]+E[1] + 0.00001))\n",
    "        non_hate_count += 1\n",
    "    secondpurity.append((E[2])/(E[0]+E[1] + E[2] + 0.00000001))\n",
    "    uniform.append(E[2])\n",
    "    non_uniform.append((E[0] + E[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo_mat = roots[['labels', 'cumm_hate_2']].values\n",
    "echo_color = {}\n",
    "for e in echo_mat:\n",
    "    echo_color[e[0]] = echo_color.get(e[0], [0, 0, 0])\n",
    "    echo_color[e[0]][0] += e[1]\n",
    "    echo_color[e[0]][1] += 1\n",
    "    if e[1] == 2:\n",
    "        echo_color[e[0]][2] += 1\n",
    "for e in echo_color:\n",
    "    echo_color[e].append(echo_color[e][2]/echo_color[e][1])\n",
    "consider_echo_colors = {}\n",
    "for c in range(len(consider_keys)):\n",
    "    vals_to_add = [0, 0]\n",
    "    for t in consider_keys[c]:\n",
    "        vals_to_add[0] += echo_color[t][1]\n",
    "        vals_to_add[1] += echo_color[t][2]\n",
    "    vals_to_add.append(vals_to_add[1]/vals_to_add[0])\n",
    "    consider_echo_colors[consider_keys[c]] = vals_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_post_fraction = []\n",
    "for c in range(len(consider_keys)):\n",
    "    hate_post_fraction.append(consider_echo_colors[consider_keys[c]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointdf = pd.DataFrame([purity, uniform, secondpurity, non_uniform, hate_post_fraction]).T\n",
    "jointdf.columns = ['purity', 'hatecount', 'secondpurity', 'non_hate_count', 'hate_post_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=jointdf, x='purity', y='hate_post_fraction', kind=\"kde\", color='green', shade=True, fill=True, cmap=\"hot\", thresh=0, levels=300)\n",
    "plt.savefig(\"GabEchoHeat.pdf\", format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointdf2 = jointdf.copy()\n",
    "jointdf2 = jointdf2[jointdf2['secondpurity'] <= 1]\n",
    "jointdf2 = jointdf2[jointdf2['purity'] <= 1]\n",
    "jointdf2 = jointdf2[jointdf2['secondpurity'] >= 0]\n",
    "jointdf2 = jointdf2[jointdf2['purity'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(data=jointdf2, x='purity', y='secondpurity', kind=\"kde\", color='red', fill=True, thresh=0, cmap='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "fig = plt.figure(figsize=(8,8), dpi=60)\n",
    "gs = GridSpec(4, 4)\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[1:4, 0:3])\n",
    "ax_hist_y = fig.add_subplot(gs[0,0:3])\n",
    "ax_hist_x = fig.add_subplot(gs[1:4, 3])\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "ax_scatter.scatter(jointdf['secondpurity'].values, jointdf['purity'].values)\n",
    "ax_hist_x.hist(jointdf['secondpurity'], orientation='horizontal', bins=20)\n",
    "ax_hist_y.hist(jointdf['purity'], bins=20)\n",
    "ax_hist_x.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "ax_hist_y.set_xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "ax_scatter.set_xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "ax_scatter.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(jointdf[['purity', 'hatecount']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.xlabel('Purity of echo chamber')\n",
    "plt.ylabel('CDF')\n",
    "for l in range(0, 1, 2):\n",
    "    X = sorted(secondpurity)\n",
    "    p = np.arange(len(X))/len(X)\n",
    "    plt.step(X, p)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity = np.array(sorted(purity))\n",
    "ax = sns.heatmap(np.array(purity).reshape(len(purity), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_PURITY = purity[purity < 0.4].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_PURITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPURE = purity[(purity > 0.5) & (purity <= 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURE = purity[(purity <= 0.5)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURE/(IMPURE+PURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_PURITY/(IMPURE+PURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_count, non_hate_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Density plots for cascade size, width, depth and velocity categorized by source post and source user\n",
    "\n",
    "Construct a CSV which is basically a list of information about each cascade in the network, with the following columns:\n",
    "1. Source post hatefulness for the cascade\n",
    "2. Source user hatefulness for the cascade\n",
    "3. Size of cascade\n",
    "4. Volume\n",
    "5. Width\n",
    "6. Height\n",
    "7. Source user in an echo chamber (yes/no)\n",
    "\n",
    "Use the following code to get density plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8), dpi=80)\n",
    "df = pd.read_csv('plot_csvs/gab_statistic.csv') # Just an example. Please generate your own CSV for the analysis you want\n",
    "# color = {\"High hate\": \"#4400aa\", \"Med hate\": \"#9955ff\", \"Non hate\": \"#ccaaff\"}\n",
    "\n",
    "# Replace <type> with any of source post hatefulness, source user hatefulness, echo chamber depending on the analysis you wish for.\n",
    "# Replace <Category Label>s with your choice depending on the number of categorit\n",
    "df.loc[df.author_type == 0, '<type>'] = '<Category Label 0>'\n",
    "df.loc[df.author_type == 1, '<type>'] = '<Category Label 1>'\n",
    "df.loc[df.author_type == 2, '<type>'] = '<Category Label 2>'\n",
    "grid = sns.kdeplot(data=df, x='<column name>', hue='<type>', common_grid=True,  log_scale=True) # Replace <column name> with any of size, width, height, volume column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code for density plots based on type of post from a type of hateful user (check Fig 1.c from manuscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('plot_csvs/gab_statistic.csv')\n",
    "log_scales = [[], []]\n",
    "df0 = df[df['source_user_hate'] == 0]\n",
    "df2 = df[df['source_user_hate'] == 2]\n",
    "\n",
    "df00 = df0[df0['source_post_hate'] == 0]\n",
    "df00.loc[df00['source_post_hate'] == 0, 'category'] = '1'\n",
    "df02 = df0[df0['source_post_hate'] == 2]\n",
    "df02.loc[df02['source_post_hate'] == 2, 'category'] = '2'\n",
    "df20 = df2[df2['source_post_hate'] == 0]\n",
    "df20.loc[df20['source_post_hate'] == 0, 'category'] = '3'\n",
    "df22 = df2[df2['source_post_hate'] == 2]\n",
    "df22.loc[df22['source_post_hate'] == 2, 'category'] = '4'\n",
    "\n",
    "DF = pd.concat([df00, df02, df20, df22])\n",
    "\n",
    "# color = {\"1\": \"#8deb91\", \"2\": \"#5dbb65\", \"3\": \"#006010\", \"4\": \"#003600\"}\n",
    "\n",
    "#Replace <column name> with any of size, width, height, volume column names\n",
    "grid = sns.kdeplot(data=DF, x='<column name>', hue='category', common_grid=True, log_scale=True, multiple='layer', linewidth=2, fill=True, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_similarity(a, b, c=0):\n",
    "    assert type(a) == set\n",
    "    assert type(b) == set\n",
    "    if c == 0:\n",
    "        # Jaccard\n",
    "        return len(a.intersection(b))/len(a.union(b))\n",
    "    else:\n",
    "        # Overlapping\n",
    "        return len(a.intersection(b))/min(len(a), len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECHO_GRAPHS = nx.Graph()\n",
    "consider_keys = list(consider_echo.keys())\n",
    "key_mappings = {}\n",
    "key_mappings_inverse = {}\n",
    "for i in range(len(consider_keys)):\n",
    "    ECHO_GRAPHS.add_node(i)\n",
    "edges = []\n",
    "for i in tqdm(range(len(consider_keys))):\n",
    "    for j in range(i+1, len(consider_keys)):\n",
    "        ### Intersecting users\n",
    "        iu_1 = set(consider_echo[consider_keys[i]])\n",
    "        iu_2 = set(consider_echo[consider_keys[j]])\n",
    "        if my_similarity(iu_1, iu_2, 1) > 0.7:\n",
    "#         if len(set(consider_keys[i]).intersection(set(consider_keys[j])))/min(len(consider_keys[i]),len(consider_keys[j])) >= 0.8:\n",
    "#         if len(set(consider_keys[i]).intersection(set(consider_keys[j])))/len(set(consider_keys[i]).union(set(consider_keys[j]))) >= 0.6:\n",
    "            ECHO_GRAPHS.add_edge(consider_keys[i],consider_keys[j])\n",
    "H = nx.relabel_nodes(ECHO_GRAPHS, key_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "from cdlib import algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removing_nodes = []\n",
    "for h in H.nodes:\n",
    "    if H.degree[h] > 3:\n",
    "        removing_nodes.append(h)\n",
    "U = nx.subgraph(H, removing_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nx.find_cliques(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncn = nx.node_clique_number(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(U, with_labels=True, font_weight='bold', node_color='lightblue', node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "degree_freq = nx.degree_histogram(H)\n",
    "degrees = range(len(degree_freq))\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.loglog(degrees[m:], degree_freq[m:],'o') \n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_echo = deepcopy(consider_echo)\n",
    "def echo_propagation(consider_keys):\n",
    "    flag = True\n",
    "    while flag:\n",
    "#         print(\"here\")\n",
    "        flag = False\n",
    "        print(flag)\n",
    "        KEYS = list(new_echo.keys())\n",
    "        for i in tqdm(range(len(KEYS))):\n",
    "            for j in range(i+1, len(KEYS)):\n",
    "                if KEYS[i] != None and KEYS[j] != None:\n",
    "                    iu_1 = set(new_echo[KEYS[i]])\n",
    "                    iu_2 = set(new_echo[KEYS[j]])\n",
    "                    iu_1_k = set(KEYS[i])\n",
    "                    iu_2_k = set(KEYS[j])\n",
    "                    if my_similarity(iu_1_k, iu_2_k, 0) >= 0.6:\n",
    "                        new_key = tuple(sorted(list(set(KEYS[i]).union(set(KEYS[j])))))\n",
    "                        new_user = iu_1.union(iu_2)\n",
    "    #                     del new_echo[KEYS[i]]\n",
    "                        del new_echo[KEYS[j]]\n",
    "    #                     print(KEYS[j])\n",
    "                        new_echo[new_key] = new_user\n",
    "                        flag = True\n",
    "                        KEYS[i] = new_key\n",
    "                        KEYS[j] = None\n",
    "                        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo_propagation(consider_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_echo = {}\n",
    "for n in new_echo:\n",
    "    for u in new_echo[n]:\n",
    "        user_to_echo[u] = user_to_echo.get(u, set())\n",
    "        user_to_echo[u].add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_echo_users = set()\n",
    "for n in new_echo:\n",
    "    all_echo_users = all_echo_users.union(new_echo[n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scales = [[], [], []]\n",
    "found = set()\n",
    "total = set()\n",
    "e_1 = 0\n",
    "e_2 = 0\n",
    "ne_1 = 0\n",
    "ne_2 = 0\n",
    "not_matched = 0\n",
    "for i in tqdm(CAS):\n",
    "    if post_author[i] in all_echo_users and author_hate_classify[post_author[i]] == 2:\n",
    "        nodes = CAS[i]\n",
    "        volume = nx.volume(casc_graph, nodes, weight=None)\n",
    "        log_scales[0].append(volume)\n",
    "        found.add(post_author[i])\n",
    "        to_compare_echo = user_to_echo[post_author[i]]\n",
    "        for n in nodes:\n",
    "            try:\n",
    "                curr_user_echo = user_to_echo.get(post_author[n], set())\n",
    "                intersect = curr_user_echo.intersection(to_compare_echo)\n",
    "                if len(intersect) > 0:\n",
    "                    e_1 += 1\n",
    "                else:\n",
    "                    e_2 += 1\n",
    "            except:\n",
    "                not_matched += 1\n",
    "    elif author_hate_classify[post_author[i]] == 2:\n",
    "        nodes = CAS[i]\n",
    "        volume = nx.volume(casc_graph, nodes, weight=None)\n",
    "        log_scales[1].append(volume)\n",
    "        total.add(post_author[i])\n",
    "        for n in nodes:\n",
    "            try:\n",
    "                if post_author[n] in all_echo_users:\n",
    "                    ne_1 += 1\n",
    "                else:\n",
    "                    ne_2 += 1\n",
    "            except:\n",
    "                not_matched += 1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_1/(e_1+e_2), ne_1/(ne_1 + ne_2), not_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [[e_1/(e_1+e_2), e_2/(e_1+e_2)],\n",
    "       [ne_1/(ne_1+ne_2), ne_2/(ne_1+ne_2)]\n",
    "      ]\n",
    "df_cm = pd.DataFrame(arr, index = [i for i in ['e1', 'e2']],\n",
    "                  columns = [i for i in ['ne1', 'ne2']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_list_echo =  []\n",
    "hate_list_all = []\n",
    "not_found = 0\n",
    "for f in total:\n",
    "    try:\n",
    "        if f in found:\n",
    "            hate_list_echo.append(author_hate_classify[f])\n",
    "        hate_list_all.append(author_hate_classify[f])\n",
    "    except:\n",
    "        not_found += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "failed = 0\n",
    "for i in tqdm(CAS):\n",
    "    desc = nx.shortest_path_length(CAS[i], i)\n",
    "    levels = {}\n",
    "    base_time = CAS[i].nodes[i]['time'].timestamp()\n",
    "    timelines = []\n",
    "    content_arr = []\n",
    "    for d in desc:\n",
    "        timelines.append(CAS[i].nodes[i]['time'].timestamp() - base_time)\n",
    "        content_arr.append(d)\n",
    "    timelines = np.array(timelines)\n",
    "    content_arr = np.array(content_arr)\n",
    "    sorter = np.argsort(timelines)\n",
    "    timelines = timelines[sorter]\n",
    "    content_arr = timelines[sorter]\n",
    "    \n",
    "    filt = timelines < 86400\n",
    "    timelines = timelines[filt]\n",
    "    content_arr = content_arr[filt]\n",
    "    vel = np.gradient(np.arange(len(content_arr)), timelines)\n",
    "    acc = np.gradient(vel, timelines)\n",
    "    zero_crossings = np.where(np.diff(np.sign(acc)))[0]\n",
    "    results[i] = [timelines, zero_crossings, CAS[i].nodes[i]['hate'], author_hate_classify[post_author[i]], int(post_author[i] in all_echo_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = np.zeros((len(results), 5))\n",
    "counter = 0\n",
    "for i in tqdm(results):\n",
    "    mappings[counter][0] = len(results[i][0])\n",
    "    mappings[counter][1] = len(results[i][1])\n",
    "    mappings[counter][2] = results[i][2]\n",
    "    mappings[counter][3] = results[i][3]\n",
    "    mappings[counter][4] = results[i][4]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_echo_hate = np.where((mappings[:, 3] == 2) & (mappings[:, 4] == 0))\n",
    "echo_hate = np.where((mappings[:, 3] == 2) & (mappings[:, 4] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[non_echo_hate][:, 0].reshape(-1, 1)\n",
    "y = mappings[non_echo_hate][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_non_echo_hate = reg.predict(X)\n",
    "X_non_echo_hate = mappings[non_echo_hate][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[non_echo_hate][:, 0], mappings[non_echo_hate][:, 1], 'o')\n",
    "plt.plot(mappings[non_echo_hate][:, 0], preds_non_echo_hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mappings[echo_hate][:, 0].reshape(-1, 1)\n",
    "y = mappings[echo_hate][:, 1].reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "preds_echo_hate = reg.predict(X)\n",
    "X_echo_hate = mappings[echo_hate][:, 0].reshape(-1, 1)\n",
    "plt.plot(mappings[echo_hate][:, 0], mappings[echo_hate][:, 1], 'o')\n",
    "plt.plot(mappings[echo_hate][:, 0], preds_echo_hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.plot(mappings[non_echo_hate][:, 0], preds_non_echo_hate, label='non echo hate')\n",
    "plt.plot(mappings[echo_hate][:, 0], preds_echo_hate, label='echo hate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BreadthFirstLevels(G,root):\n",
    "    \"\"\"\n",
    "    Generate a sequence of bipartite directed graphs, each consisting\n",
    "    of the edges from level i to level i+1 of G. Edges that connect\n",
    "    vertices within the same level are not included in the output.\n",
    "    The vertices in each level can be listed by iterating over each\n",
    "    output graph.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    currentLevel = [root]\n",
    "    while currentLevel:\n",
    "        for v in currentLevel:\n",
    "            visited.add(v)\n",
    "        nextLevel = set()\n",
    "        levelGraph = {v:set() for v in currentLevel}\n",
    "        for v in currentLevel:\n",
    "            for w in G[v]:\n",
    "                if w not in visited:\n",
    "                    levelGraph[v].add(w)\n",
    "                    nextLevel.add(w)\n",
    "        yield nextLevel\n",
    "        currentLevel = nextLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_plots = {\n",
    "    0: {\n",
    "        'h': {}\n",
    "    },\n",
    "    1: {\n",
    "        'h': {}\n",
    "    },\n",
    "    2: {\n",
    "        'h': {}\n",
    "    } \n",
    "}\n",
    "\n",
    "for i in tqdm(CAS):\n",
    "    h = 1\n",
    "    arr = [[h, 1]]\n",
    "    bfs = BreadthFirstLevels(CAS[i],i)\n",
    "    hate_index = CAS[i].nodes[i]['hate']\n",
    "#     hate_index = author_hate_classify[P_dict[i]['author']]\n",
    "    for b in bfs:\n",
    "        h += 1\n",
    "        dict_plots[hate_index]['h'][h] = dict_plots[hate_index]['h'].get(h, [])\n",
    "        dict_plots[hate_index]['h'][h].append(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE_TO_SHOW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_strip = list()\n",
    "for i in range(0, 3):\n",
    "    for j in dict_plots[0]['h']:\n",
    "        for k in dict_plots[0]['h'][j]:\n",
    "            if j <= RANGE_TO_SHOW:\n",
    "                dict_to_strip.append([i, # species\n",
    "                                      j, # depth\n",
    "                                      k # width\n",
    "                                     ])\n",
    "dict_to_strip = np.array(dict_to_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_cascade = pd.DataFrame(columns=['hate', 'depth', 'width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_cascade['hate'] = dict_to_strip[:, 0]\n",
    "size_cascade['depth'] = dict_to_strip[:, 1]\n",
    "size_cascade['width'] = dict_to_strip[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.stripplot(x=\"depth\", y=\"width\", hue=\"hate\",\n",
    "              data=size_cascade, dodge=True, alpha=.25, zorder=1)\n",
    "sns.pointplot(x=\"depth\", y=\"width\", hue=\"hate\",\n",
    "              data=size_cascade, dodge=.8 - .8 / 3,\n",
    "              join=False, palette=\"dark\",\n",
    "              markers=\"d\", scale=.75, ci=None)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[3:], labels[3:], title=\"Hate intensity\",\n",
    "          handletextpad=0, columnspacing=1,\n",
    "          loc=\"lower right\", ncol=3, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=\"depth\", y=\"width\", hue=\"hate\",\n",
    "              data=size_cascade,\n",
    "    kind=\"kde\", height=6,\n",
    "    multiple=\"fill\", clip=(0, None),\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.75\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"depth\", y=\"width\", hue=\"hate\",\n",
    "               data=size_cascade, kind=\"violin\", aspect=2, height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "g = sns.catplot(x=\"depth\", y=\"width\", hue=\"hate\",\n",
    "                capsize=.2, palette=\"Reds\",  aspect=.75,\n",
    "                kind=\"point\", data=size_cascade, width=20, height=5)\n",
    "g.despine(left=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
